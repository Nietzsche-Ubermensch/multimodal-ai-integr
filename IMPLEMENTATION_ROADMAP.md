# AI Integration Platform - Implementation Roadmap
## Comprehensive System Architecture & Enhancement Plan

> **Status**: Production-Ready Foundation | **Next Phase**: Enterprise Scale & Advanced Features
> **Current Version**: 1.0.0 | **Target Version**: 2.0.0

---

## Executive Summary

This document provides a structured roadmap for evolving the AI Integration Platform from its current comprehensive state into an enterprise-grade, production-ready system with advanced capabilities. The platform currently supports:

- âœ… **8+ AI Providers**: Anthropic, DeepSeek, xAI, OpenRouter, LiteLLM, Firecrawl, Oxylabs, Supabase
- âœ… **End-to-End RAG Pipeline**: Web scraping â†’ Embedding â†’ Vector storage â†’ Retrieval â†’ Generation
- âœ… **Real-Time API Testing**: Live validation across all providers
- âœ… **Interactive SDK Demos**: Working examples for all major providers
- âœ… **Comprehensive Documentation**: Setup guides, deployment instructions, best practices

### Key Enhancement Areas

1. **Backend Microservices Architecture** (Priority: High)
2. **API Gateway & Request Routing** (Priority: High)  
3. **Database Integration & Persistence** (Priority: Medium)
4. **Advanced Security & Key Management** (Priority: High)
5. **Production Monitoring & Observability** (Priority: Medium)
6. **Scalability & Performance Optimization** (Priority: Medium)

---

## Phase 1: Backend Architecture Evolution (Weeks 1-4)

### Current State Analysis

**Frontend**: React + TypeScript (âœ… Complete)
- Comprehensive UI with tab-based navigation
- Interactive API testing components
- Real-time status monitoring
- Live SDK demos for all providers

**Backend**: **Missing** - Currently client-side only
- âŒ No API gateway for secure routing
- âŒ No centralized key management
- âŒ No request logging/auditing
- âŒ No rate limiting infrastructure
- âŒ No database persistence layer

### 1.1 Microservices Architecture Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend Layer                           â”‚
â”‚              (React + TypeScript - EXISTING)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  API     â”‚  â”‚  RAG     â”‚  â”‚  Status  â”‚  â”‚  SDK     â”‚        â”‚
â”‚  â”‚  Tester  â”‚  â”‚  Demo    â”‚  â”‚  Monitor â”‚  â”‚  Demos   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       API Gateway (NEW)                â”‚
        â”‚  - Route Management                    â”‚
        â”‚  - Authentication/Authorization        â”‚
        â”‚  - Rate Limiting                       â”‚
        â”‚  - Request/Response Logging            â”‚
        â”‚  - Load Balancing                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Provider    â”‚  â”‚   RAG       â”‚  â”‚   Key       â”‚  â”‚   Analytics â”‚
â”‚   Service     â”‚  â”‚   Service   â”‚  â”‚   Service   â”‚  â”‚   Service   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Anthropic   â”‚  â”‚ â€¢ Oxylabs   â”‚  â”‚ â€¢ Encrypt   â”‚  â”‚ â€¢ Metrics   â”‚
â”‚ â€¢ DeepSeek    â”‚  â”‚ â€¢ Supabase  â”‚  â”‚ â€¢ Validate  â”‚  â”‚ â€¢ Logs      â”‚
â”‚ â€¢ xAI         â”‚  â”‚ â€¢ LiteLLM   â”‚  â”‚ â€¢ Rotate    â”‚  â”‚ â€¢ Alerts    â”‚
â”‚ â€¢ OpenRouter  â”‚  â”‚ â€¢ Embeddingsâ”‚  â”‚ â€¢ Audit     â”‚  â”‚ â€¢ Reports   â”‚
â”‚ â€¢ NVIDIA NIM  â”‚  â”‚ â€¢ Chunking  â”‚  â”‚             â”‚  â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Data Layer (Supabase)     â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ â€¢ User Configurations         â”‚
                    â”‚ â€¢ API Keys (Encrypted)        â”‚
                    â”‚ â€¢ Request Logs                â”‚
                    â”‚ â€¢ Vector Embeddings           â”‚
                    â”‚ â€¢ Usage Metrics               â”‚
                    â”‚ â€¢ Audit Trail                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Technology Stack Recommendations

#### Backend Services (Choose One Path)

**Option A: Node.js/TypeScript Stack** (Recommended for React developers)
```typescript
// Technology Stack
- Runtime: Node.js 20+ / Bun 1.0+
- Framework: Fastify (fastest) or Express
- API Gateway: NGINX + Node.js middleware
- Database Client: @supabase/supabase-js
- Validation: Zod
- Testing: Vitest + Supertest
- Containerization: Docker + Docker Compose
```

**Option B: Python Stack** (Recommended for AI/ML teams)
```python
# Technology Stack
- Runtime: Python 3.11+
- Framework: FastAPI (async, OpenAPI auto-gen)
- API Gateway: Traefik or Kong
- Database Client: supabase-py
- Validation: Pydantic
- Testing: pytest + httpx
- Containerization: Docker + Docker Compose
```

**Option C: Hybrid Stack** (Best of both worlds)
```
- API Gateway: Node.js (Fastify) - Fast routing, JS/TS consistency
- RAG Service: Python (FastAPI) - Better for ML/embeddings
- Provider Service: Node.js (TypeScript) - SDK compatibility
- Key Service: Python (FastAPI) - Cryptography libraries
```

### 1.3 API Gateway Implementation

#### Core Responsibilities

1. **Request Routing**
   - `/api/v1/providers/*` â†’ Provider Service
   - `/api/v1/rag/*` â†’ RAG Service
   - `/api/v1/keys/*` â†’ Key Management Service
   - `/api/v1/analytics/*` â†’ Analytics Service

2. **Authentication & Authorization**
   - JWT token validation
   - API key authentication for external integrations
   - Role-based access control (RBAC)
   - Session management

3. **Rate Limiting**
   - Per-user quotas
   - Per-endpoint limits
   - Provider-specific throttling
   - Burst protection

4. **Request/Response Transformation**
   - Standardized error formats
   - Response compression (gzip/brotli)
   - Request validation
   - CORS handling

#### Example: API Gateway (Node.js/Fastify)

```typescript
// src/gateway/server.ts
import Fastify from 'fastify';
import cors from '@fastify/cors';
import rateLimit from '@fastify/rate-limit';
import jwt from '@fastify/jwt';
import { TypeBoxTypeProvider } from '@fastify/type-provider-typebox';
import { Type } from '@sinclair/typebox';

const server = Fastify({
  logger: true,
}).withTypeProvider<TypeBoxTypeProvider>();

// Middleware
await server.register(cors, { 
  origin: process.env.ALLOWED_ORIGINS?.split(',') || ['http://localhost:5173']
});

await server.register(jwt, {
  secret: process.env.JWT_SECRET!
});

await server.register(rateLimit, {
  max: 100,
  timeWindow: '1 minute',
  redis: {
    host: process.env.REDIS_HOST || 'localhost',
    port: parseInt(process.env.REDIS_PORT || '6379')
  }
});

// Health Check
server.get('/health', async () => {
  return { 
    status: 'healthy', 
    timestamp: new Date().toISOString(),
    version: '1.0.0'
  };
});

// Provider Proxy Routes
server.register(async (fastify) => {
  fastify.addHook('onRequest', async (request, reply) => {
    try {
      await request.jwtVerify();
    } catch (err) {
      reply.send(err);
    }
  });

  // Anthropic
  fastify.post('/api/v1/providers/anthropic/chat', {
    schema: {
      body: Type.Object({
        model: Type.String(),
        messages: Type.Array(Type.Object({
          role: Type.String(),
          content: Type.String()
        })),
        temperature: Type.Optional(Type.Number({ minimum: 0, maximum: 2 })),
        max_tokens: Type.Optional(Type.Integer({ minimum: 1 }))
      })
    }
  }, async (request, reply) => {
    // Forward to Anthropic via internal service
    const response = await fetch(`http://provider-service:3001/anthropic/chat`, {
      method: 'POST',
      headers: { 
        'Content-Type': 'application/json',
        'X-User-ID': request.user.sub
      },
      body: JSON.stringify(request.body)
    });
    
    return response.json();
  });

  // OpenRouter (similar pattern)
  fastify.post('/api/v1/providers/openrouter/chat', async (request, reply) => {
    // Implementation...
  });

  // RAG Pipeline
  fastify.post('/api/v1/rag/execute', {
    schema: {
      body: Type.Object({
        url: Type.String({ format: 'uri' }),
        query: Type.String(),
        provider: Type.String()
      })
    }
  }, async (request, reply) => {
    const response = await fetch(`http://rag-service:3002/execute`, {
      method: 'POST',
      headers: { 
        'Content-Type': 'application/json',
        'X-User-ID': request.user.sub
      },
      body: JSON.stringify(request.body)
    });
    
    return response.json();
  });
});

// Start server
await server.listen({ port: 3000, host: '0.0.0.0' });
console.log('API Gateway running on port 3000');
```

#### Example: API Gateway (Python/FastAPI)

```python
# src/gateway/main.py
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from pydantic import BaseModel, HttpUrl
import httpx
import os
from typing import List, Optional

app = FastAPI(
    title="AI Integration Platform API",
    version="1.0.0",
    docs_url="/api/docs"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("ALLOWED_ORIGINS", "http://localhost:5173").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Compression
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Rate Limiting
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None

class RAGRequest(BaseModel):
    url: HttpUrl
    query: str
    provider: str

# Health Check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "1.0.0"
    }

# Provider Routes
@app.post("/api/v1/providers/anthropic/chat")
@limiter.limit("60/minute")
async def anthropic_chat(request: ChatRequest):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://provider-service:3001/anthropic/chat",
            json=request.dict(),
            timeout=30.0
        )
        return response.json()

@app.post("/api/v1/providers/openrouter/chat")
@limiter.limit("60/minute")
async def openrouter_chat(request: ChatRequest):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://provider-service:3001/openrouter/chat",
            json=request.dict(),
            timeout=30.0
        )
        return response.json()

# RAG Pipeline
@app.post("/api/v1/rag/execute")
@limiter.limit("10/minute")
async def execute_rag_pipeline(request: RAGRequest):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://rag-service:3002/execute",
            json=request.dict(),
            timeout=120.0  # RAG can take longer
        )
        return response.json()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3000)
```

---

## Phase 2: Database Integration (Weeks 3-5)

### 2.1 Supabase Schema Design

```sql
-- Enable required extensions
create extension if not exists "uuid-ossp";
create extension if not exists "vector";

-- Users table
create table public.users (
  id uuid primary key default uuid_generate_v4(),
  email text unique not null,
  created_at timestamp with time zone default now(),
  updated_at timestamp with time zone default now(),
  role text not null default 'user' check (role in ('user', 'admin')),
  metadata jsonb default '{}'::jsonb
);

-- API Keys table (encrypted storage)
create table public.api_keys (
  id uuid primary key default uuid_generate_v4(),
  user_id uuid references public.users(id) on delete cascade,
  provider text not null check (provider in (
    'anthropic', 'deepseek', 'xai', 'openrouter', 
    'nvidia_nim', 'openai', 'firecrawl', 'oxylabs', 'supabase'
  )),
  key_name text not null,
  encrypted_key text not null, -- Encrypted with Supabase Vault
  is_active boolean default true,
  last_validated_at timestamp with time zone,
  validation_status text check (validation_status in ('valid', 'invalid', 'pending')),
  created_at timestamp with time zone default now(),
  updated_at timestamp with time zone default now(),
  unique(user_id, provider)
);

-- Request Logs
create table public.request_logs (
  id uuid primary key default uuid_generate_v4(),
  user_id uuid references public.users(id) on delete set null,
  provider text not null,
  endpoint text not null,
  method text not null,
  status_code integer,
  latency_ms integer,
  tokens_used integer,
  cost_usd numeric(10, 6),
  error_message text,
  metadata jsonb default '{}'::jsonb,
  created_at timestamp with time zone default now()
);

-- Vector Documents (for RAG)
create table public.documents (
  id uuid primary key default uuid_generate_v4(),
  user_id uuid references public.users(id) on delete cascade,
  source_url text not null,
  title text,
  content text not null,
  chunk_index integer not null,
  total_chunks integer not null,
  embedding vector(1536), -- OpenAI ada-002 dimension
  metadata jsonb default '{}'::jsonb,
  created_at timestamp with time zone default now()
);

-- Create index for vector similarity search
create index on public.documents 
using ivfflat (embedding vector_cosine_ops)
with (lists = 100);

-- Usage Analytics
create table public.usage_analytics (
  id uuid primary key default uuid_generate_v4(),
  user_id uuid references public.users(id) on delete cascade,
  date date not null,
  provider text not null,
  total_requests integer default 0,
  total_tokens integer default 0,
  total_cost_usd numeric(10, 6) default 0,
  metadata jsonb default '{}'::jsonb,
  unique(user_id, date, provider)
);

-- Row Level Security (RLS) Policies

-- Users can only see their own data
alter table public.users enable row level security;
create policy "Users can view own data" on public.users
  for select using (auth.uid() = id);

alter table public.api_keys enable row level security;
create policy "Users can manage own keys" on public.api_keys
  for all using (auth.uid() = user_id);

alter table public.request_logs enable row level security;
create policy "Users can view own logs" on public.request_logs
  for select using (auth.uid() = user_id);

alter table public.documents enable row level security;
create policy "Users can manage own documents" on public.documents
  for all using (auth.uid() = user_id);

alter table public.usage_analytics enable row level security;
create policy "Users can view own analytics" on public.usage_analytics
  for select using (auth.uid() = user_id);

-- Functions

-- Vector similarity search
create or replace function match_documents (
  query_embedding vector(1536),
  match_threshold float default 0.78,
  match_count int default 5,
  filter_user_id uuid default null
)
returns table (
  id uuid,
  content text,
  similarity float,
  metadata jsonb
)
language plpgsql
as $$
begin
  return query
  select
    documents.id,
    documents.content,
    1 - (documents.embedding <=> query_embedding) as similarity,
    documents.metadata
  from public.documents
  where 
    (filter_user_id is null or documents.user_id = filter_user_id)
    and 1 - (documents.embedding <=> query_embedding) > match_threshold
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Update timestamps automatically
create or replace function update_updated_at_column()
returns trigger as $$
begin
  new.updated_at = now();
  return new;
end;
$$ language plpgsql;

create trigger update_users_updated_at before update on public.users
  for each row execute procedure update_updated_at_column();

create trigger update_api_keys_updated_at before update on public.api_keys
  for each row execute procedure update_updated_at_column();
```

### 2.2 Database Service Implementation

```typescript
// src/services/database.service.ts
import { createClient, SupabaseClient } from '@supabase/supabase-js';

export interface Database {
  public: {
    Tables: {
      users: { /* ... */ };
      api_keys: { /* ... */ };
      request_logs: { /* ... */ };
      documents: { /* ... */ };
      usage_analytics: { /* ... */ };
    };
  };
}

export class DatabaseService {
  private supabase: SupabaseClient<Database>;

  constructor() {
    this.supabase = createClient<Database>(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY! // Use service role for server-side
    );
  }

  // API Key Management
  async storeApiKey(
    userId: string, 
    provider: string, 
    encryptedKey: string
  ): Promise<void> {
    const { error } = await this.supabase
      .from('api_keys')
      .upsert({
        user_id: userId,
        provider,
        encrypted_key: encryptedKey,
        key_name: `${provider}_api_key`,
        validation_status: 'pending'
      });

    if (error) throw error;
  }

  async getApiKey(userId: string, provider: string): Promise<string | null> {
    const { data, error } = await this.supabase
      .from('api_keys')
      .select('encrypted_key')
      .eq('user_id', userId)
      .eq('provider', provider)
      .eq('is_active', true)
      .single();

    if (error || !data) return null;
    return this.decryptKey(data.encrypted_key);
  }

  // Request Logging
  async logRequest(log: {
    userId: string;
    provider: string;
    endpoint: string;
    method: string;
    statusCode: number;
    latencyMs: number;
    tokensUsed?: number;
    costUsd?: number;
    errorMessage?: string;
    metadata?: any;
  }): Promise<void> {
    const { error } = await this.supabase
      .from('request_logs')
      .insert({
        user_id: log.userId,
        provider: log.provider,
        endpoint: log.endpoint,
        method: log.method,
        status_code: log.statusCode,
        latency_ms: log.latencyMs,
        tokens_used: log.tokensUsed,
        cost_usd: log.costUsd,
        error_message: log.errorMessage,
        metadata: log.metadata || {}
      });

    if (error) console.error('Failed to log request:', error);
  }

  // Vector Document Management
  async storeDocuments(
    userId: string,
    sourceUrl: string,
    chunks: Array<{ content: string; embedding: number[]; index: number }>
  ): Promise<void> {
    const documents = chunks.map(chunk => ({
      user_id: userId,
      source_url: sourceUrl,
      content: chunk.content,
      chunk_index: chunk.index,
      total_chunks: chunks.length,
      embedding: chunk.embedding,
      metadata: { source: 'rag_pipeline' }
    }));

    const { error } = await this.supabase
      .from('documents')
      .insert(documents);

    if (error) throw error;
  }

  async searchSimilarDocuments(
    userId: string,
    queryEmbedding: number[],
    matchCount: number = 5,
    threshold: number = 0.78
  ): Promise<Array<{ content: string; similarity: number }>> {
    const { data, error } = await this.supabase
      .rpc('match_documents', {
        query_embedding: queryEmbedding,
        match_threshold: threshold,
        match_count: matchCount,
        filter_user_id: userId
      });

    if (error) throw error;
    return data || [];
  }

  // Analytics
  async recordUsage(
    userId: string,
    provider: string,
    tokens: number,
    costUsd: number
  ): Promise<void> {
    const today = new Date().toISOString().split('T')[0];

    const { error } = await this.supabase.rpc('increment_usage', {
      p_user_id: userId,
      p_date: today,
      p_provider: provider,
      p_tokens: tokens,
      p_cost: costUsd
    });

    if (error) console.error('Failed to record usage:', error);
  }

  private decryptKey(encrypted: string): string {
    // Implement decryption using Supabase Vault or KMS
    // For now, placeholder
    return encrypted;
  }
}
```

---

## Phase 3: Security & Key Management (Weeks 4-6)

### 3.1 API Key Encryption Strategy

**Current State**: API keys stored in browser localStorage (insecure for production)

**Target State**: Server-side encrypted storage with Supabase Vault or AWS KMS

#### Implementation Options

**Option A: Supabase Vault (Simpler)**
```sql
-- Enable Vault extension
create extension if not exists vault cascade;

-- Create encryption key
select vault.create_secret(
  'my-app-key-encryption-key',
  'your-32-byte-encryption-key-here'
);

-- Encrypt API key before storage
insert into public.api_keys (user_id, provider, encrypted_key)
values (
  'user-uuid',
  'anthropic',
  vault.encrypt('sk-ant-api03-...', 'my-app-key-encryption-key')
);

-- Decrypt when retrieving
select 
  provider,
  vault.decrypt(encrypted_key, 'my-app-key-encryption-key') as decrypted_key
from public.api_keys
where user_id = 'user-uuid';
```

**Option B: AWS KMS (Enterprise-grade)**
```typescript
import { KMSClient, EncryptCommand, DecryptCommand } from '@aws-sdk/client-kms';

export class KeyEncryptionService {
  private kms: KMSClient;
  private keyId: string;

  constructor() {
    this.kms = new KMSClient({ region: process.env.AWS_REGION });
    this.keyId = process.env.KMS_KEY_ID!;
  }

  async encrypt(plaintext: string): Promise<string> {
    const command = new EncryptCommand({
      KeyId: this.keyId,
      Plaintext: Buffer.from(plaintext)
    });

    const response = await this.kms.send(command);
    return Buffer.from(response.CiphertextBlob!).toString('base64');
  }

  async decrypt(ciphertext: string): Promise<string> {
    const command = new DecryptCommand({
      CiphertextBlob: Buffer.from(ciphertext, 'base64')
    });

    const response = await this.kms.send(command);
    return Buffer.from(response.Plaintext!).toString('utf-8');
  }
}
```

### 3.2 Authentication Flow

```typescript
// src/middleware/auth.ts
import { FastifyRequest, FastifyReply } from 'fastify';
import { createClient } from '@supabase/supabase-js';

export async function authenticateUser(
  request: FastifyRequest,
  reply: FastifyReply
) {
  const token = request.headers.authorization?.replace('Bearer ', '');

  if (!token) {
    return reply.code(401).send({ error: 'No token provided' });
  }

  const supabase = createClient(
    process.env.SUPABASE_URL!,
    process.env.SUPABASE_ANON_KEY!
  );

  const { data: { user }, error } = await supabase.auth.getUser(token);

  if (error || !user) {
    return reply.code(401).send({ error: 'Invalid token' });
  }

  // Attach user to request
  request.user = user;
}
```

### 3.3 Rate Limiting Strategy

```typescript
// src/config/rate-limits.ts
export const rateLimits = {
  global: {
    max: 1000,
    timeWindow: '1 hour'
  },
  byEndpoint: {
    '/api/v1/providers/*/chat': {
      max: 60,
      timeWindow: '1 minute'
    },
    '/api/v1/rag/execute': {
      max: 10,
      timeWindow: '1 minute'
    },
    '/api/v1/keys/*': {
      max: 20,
      timeWindow: '1 hour'
    }
  },
  byUser: {
    free: {
      max: 100,
      timeWindow: '1 day'
    },
    pro: {
      max: 1000,
      timeWindow: '1 day'
    },
    enterprise: {
      max: 10000,
      timeWindow: '1 day'
    }
  }
};
```

---

## Phase 4: Monitoring & Observability (Weeks 5-7)

### 4.1 Metrics Collection

```typescript
// src/services/metrics.service.ts
import { Registry, Counter, Histogram, Gauge } from 'prom-client';

export class MetricsService {
  private registry: Registry;
  private requestCounter: Counter;
  private requestDuration: Histogram;
  private activeRequests: Gauge;
  private errorCounter: Counter;

  constructor() {
    this.registry = new Registry();

    this.requestCounter = new Counter({
      name: 'http_requests_total',
      help: 'Total number of HTTP requests',
      labelNames: ['method', 'route', 'status_code', 'provider'],
      registers: [this.registry]
    });

    this.requestDuration = new Histogram({
      name: 'http_request_duration_seconds',
      help: 'Duration of HTTP requests in seconds',
      labelNames: ['method', 'route', 'provider'],
      buckets: [0.1, 0.5, 1, 2, 5, 10],
      registers: [this.registry]
    });

    this.activeRequests = new Gauge({
      name: 'http_requests_active',
      help: 'Number of active HTTP requests',
      labelNames: ['method', 'route'],
      registers: [this.registry]
    });

    this.errorCounter = new Counter({
      name: 'http_errors_total',
      help: 'Total number of HTTP errors',
      labelNames: ['method', 'route', 'error_type'],
      registers: [this.registry]
    });
  }

  recordRequest(method: string, route: string, statusCode: number, provider?: string) {
    this.requestCounter.inc({ method, route, status_code: statusCode, provider: provider || 'none' });
  }

  recordDuration(method: string, route: string, durationSeconds: number, provider?: string) {
    this.requestDuration.observe({ method, route, provider: provider || 'none' }, durationSeconds);
  }

  incrementActive(method: string, route: string) {
    this.activeRequests.inc({ method, route });
  }

  decrementActive(method: string, route: string) {
    this.activeRequests.dec({ method, route });
  }

  recordError(method: string, route: string, errorType: string) {
    this.errorCounter.inc({ method, route, error_type: errorType });
  }

  async getMetrics(): Promise<string> {
    return this.registry.metrics();
  }
}
```

### 4.2 Logging Strategy

```typescript
// src/services/logger.service.ts
import pino from 'pino';

export const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  transport: process.env.NODE_ENV === 'development' 
    ? { target: 'pino-pretty' }
    : undefined,
  formatters: {
    level: (label) => {
      return { level: label.toUpperCase() };
    },
  },
  timestamp: pino.stdTimeFunctions.isoTime,
});

// Structured logging helper
export function logRequest(data: {
  method: string;
  url: string;
  userId?: string;
  provider?: string;
  statusCode: number;
  durationMs: number;
  error?: string;
}) {
  logger.info({
    type: 'http_request',
    ...data
  });
}

export function logApiCall(data: {
  provider: string;
  endpoint: string;
  userId: string;
  latencyMs: number;
  tokensUsed?: number;
  costUsd?: number;
  success: boolean;
  errorMessage?: string;
}) {
  logger.info({
    type: 'api_call',
    ...data
  });
}
```

---

## Phase 5: Docker Deployment (Week 6)

### 5.1 Docker Compose Configuration

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Frontend (React/Vite)
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_GATEWAY_URL=http://api-gateway:3000
    depends_on:
      - api-gateway
    networks:
      - ai-platform

  # API Gateway
  api-gateway:
    build:
      context: ./services/gateway
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - JWT_SECRET=${JWT_SECRET}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - redis
      - provider-service
      - rag-service
    networks:
      - ai-platform

  # Provider Service (handles all AI provider integrations)
  provider-service:
    build:
      context: ./services/provider
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - XAI_API_KEY=${XAI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - NVIDIA_NIM_API_KEY=${NVIDIA_NIM_API_KEY}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
    networks:
      - ai-platform

  # RAG Service (Python-based for better ML support)
  rag-service:
    build:
      context: ./services/rag
      dockerfile: Dockerfile
    environment:
      - OXYLABS_USERNAME=${OXYLABS_USERNAME}
      - OXYLABS_PASSWORD=${OXYLABS_PASSWORD}
      - FIRECRAWL_API_KEY=${FIRECRAWL_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
    networks:
      - ai-platform

  # Redis (for caching and rate limiting)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - ai-platform

  # Prometheus (metrics)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    networks:
      - ai-platform

  # Grafana (dashboards)
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana-dashboards:/etc/grafana/provisioning/dashboards
    depends_on:
      - prometheus
    networks:
      - ai-platform

networks:
  ai-platform:
    driver: bridge

volumes:
  redis-data:
  prometheus-data:
  grafana-data:
```

### 5.2 Individual Dockerfiles

```dockerfile
# Dockerfile.frontend
FROM node:20-alpine AS builder

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

```dockerfile
# services/gateway/Dockerfile
FROM node:20-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 3000
CMD ["node", "dist/server.js"]
```

```dockerfile
# services/rag/Dockerfile (Python)
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 3002
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "3002"]
```

---

## Phase 6: Production Deployment Strategy (Week 7-8)

### 6.1 Cloud Platform Options

#### Option A: AWS

```yaml
# AWS Architecture
- Frontend: S3 + CloudFront
- API Gateway: ALB + ECS Fargate
- Services: ECS Fargate (auto-scaling)
- Database: Supabase (managed)
- Caching: ElastiCache (Redis)
- Secrets: AWS Secrets Manager
- Monitoring: CloudWatch + Grafana Cloud
- CDN: CloudFront
- DNS: Route 53
```

#### Option B: Google Cloud Platform

```yaml
# GCP Architecture
- Frontend: Cloud Storage + Cloud CDN
- API Gateway: Cloud Run
- Services: Cloud Run (auto-scaling containers)
- Database: Supabase (managed)
- Caching: Memorystore (Redis)
- Secrets: Secret Manager
- Monitoring: Cloud Monitoring + Grafana
- CDN: Cloud CDN
- DNS: Cloud DNS
```

#### Option C: Vercel + Serverless

```yaml
# Vercel Architecture
- Frontend: Vercel (edge network)
- API Gateway: Vercel Edge Functions
- Services: Vercel Serverless Functions
- Database: Supabase (managed)
- Caching: Vercel KV (Redis)
- Secrets: Vercel Environment Variables
- Monitoring: Vercel Analytics + Axiom
```

### 6.2 CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy AI Platform

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '20'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Run tests
        run: npm test
        
      - name: Run linter
        run: npm run lint

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Build Docker images
        run: |
          docker-compose build
          
      - name: Tag images
        run: |
          docker tag ai-platform-frontend:latest ${{ secrets.DOCKER_REGISTRY }}/frontend:${{ github.sha }}
          docker tag ai-platform-gateway:latest ${{ secrets.DOCKER_REGISTRY }}/gateway:${{ github.sha }}
          
      - name: Push to registry
        run: |
          docker push ${{ secrets.DOCKER_REGISTRY }}/frontend:${{ github.sha }}
          docker push ${{ secrets.DOCKER_REGISTRY }}/gateway:${{ github.sha }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to production
        run: |
          # Your deployment commands here
          # e.g., kubectl apply, terraform apply, etc.
```

---

## Implementation Timeline Summary

| Phase | Duration | Priority | Key Deliverables |
|-------|----------|----------|------------------|
| **Phase 1: Backend Architecture** | 4 weeks | High | API Gateway, Microservices, Basic routing |
| **Phase 2: Database Integration** | 2-3 weeks | Medium | Supabase schema, Data persistence, RLS policies |
| **Phase 3: Security & Key Management** | 2-3 weeks | High | Encryption, Authentication, Rate limiting |
| **Phase 4: Monitoring & Observability** | 2-3 weeks | Medium | Metrics, Logging, Dashboards |
| **Phase 5: Docker Deployment** | 1 week | Medium | Docker Compose, Local deployment |
| **Phase 6: Production Deployment** | 1-2 weeks | High | Cloud deployment, CI/CD pipeline |

**Total Estimated Timeline**: 12-16 weeks (3-4 months)

---

## Next Steps & Recommendations

### Immediate Actions (Week 1)

1. **Choose Technology Stack** â†’ Node.js/TypeScript recommended for consistency
2. **Set up Supabase Project** â†’ Create production database with schema
3. **Implement Basic API Gateway** â†’ Start with health checks and provider routing
4. **Configure Environment Variables** â†’ Secure secrets management setup

### Priority Features for v2.0

1. âœ… **Backend API Gateway** (Highest Priority)
2. âœ… **Supabase Integration** (High Priority)
3. âœ… **API Key Encryption** (High Priority - Security)
4. âš ï¸ **User Authentication** (Medium Priority - Can use existing)
5. âš ï¸ **Request Logging** (Medium Priority)
6. ğŸ”„ **Advanced Monitoring** (Lower Priority - Phase 2)

### Questions for Stakeholders

1. **Preferred backend stack?** Node.js/TypeScript, Python, or Hybrid?
2. **Cloud platform preference?** AWS, GCP, or Vercel?
3. **Budget constraints?** Affects provider selection and scaling strategy
4. **Compliance requirements?** GDPR, HIPAA, SOC2?
5. **Expected user scale?** 100s, 1000s, or 10,000s of daily active users?
6. **Priority: Speed vs Features?** MVP in 4 weeks or full system in 16 weeks?

---

## Conclusion

This roadmap provides a comprehensive path from the current client-side-only implementation to a production-ready, enterprise-grade AI Integration Platform. The phased approach allows for iterative development, early testing, and continuous deployment while maintaining the existing functionality.

**Key Success Factors:**
- âœ… Build on existing comprehensive frontend
- âœ… Incremental backend implementation
- âœ… Security-first approach
- âœ… Scalable architecture from day one
- âœ… Comprehensive monitoring and observability

**Document Status**: Living document - will be updated as implementation progresses.

**Last Updated**: 2025-01-XX
**Version**: 1.0.0
**Next Review**: After Phase 1 completion
