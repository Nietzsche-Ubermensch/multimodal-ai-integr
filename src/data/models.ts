export interface ModelCapability {
  name: string;
  supported: boolean;
}

export interface Model {
  id: string;
  name: string;
  provider: "xai" | "deepseek" | "anthropic" | "openrouter" | "huggingface" | "openai" | "nvidia";
  contextWindow: number;
  inputCostPer1M: number;
  outputCostPer1M: number;
  modelType: "chat" | "reasoning" | "code" | "vision" | "embedding";
  capabilities: ModelCapability[];
  description: string;
  tags: string[];
  maxTokens?: number;
  supportsStreaming: boolean;
  supportsFunctionCalling: boolean;
  supportsVision: boolean;
}

export const UNIFIED_MODEL_CATALOG: Model[] = [
  {
    id: "xai/grok-4-1-fast-reasoning",
    name: "Grok 4.1 Fast (Reasoning)",
    provider: "xai",
    contextWindow: 2_000_000,
    inputCostPer1M: 15.0,
    outputCostPer1M: 30.0,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "reasoning", supported: true },
      { name: "web_search", supported: true },
      { name: "tool_calling", supported: true },
    ],
    description: "Next-gen reasoning model with 2M context and agent tools",
    tags: ["frontier", "reasoning", "web-search", "2M-context"],
    maxTokens: 4096,
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "xai/grok-4-1-fast-non-reasoning",
    name: "Grok 4.1 Fast",
    provider: "xai",
    contextWindow: 2_000_000,
    inputCostPer1M: 8.0,
    outputCostPer1M: 16.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "web_search", supported: true },
    ],
    description: "Fast chat model with 2M context",
    tags: ["fast", "chat", "web-search", "2M-context"],
    maxTokens: 4096,
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "xai/grok-code-fast-1",
    name: "Grok Code Fast",
    provider: "xai",
    contextWindow: 2_000_000,
    inputCostPer1M: 12.0,
    outputCostPer1M: 24.0,
    modelType: "code",
    capabilities: [
      { name: "code", supported: true },
      { name: "agent", supported: true },
    ],
    description: "Code-specialized model with agent capabilities",
    tags: ["code", "agent", "fast"],
    maxTokens: 4096,
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "xai/grok-2-vision-latest",
    name: "Grok 2 Vision",
    provider: "xai",
    contextWindow: 32_000,
    inputCostPer1M: 10.0,
    outputCostPer1M: 20.0,
    modelType: "vision",
    capabilities: [
      { name: "text", supported: true },
      { name: "vision", supported: true },
    ],
    description: "Multimodal vision model",
    tags: ["vision", "multimodal"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: true,
  },
  {
    id: "deepseek/deepseek-chat",
    name: "DeepSeek Chat V3.2",
    provider: "deepseek",
    contextWindow: 128_000,
    inputCostPer1M: 0.14,
    outputCostPer1M: 0.28,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "thinking", supported: true },
    ],
    description: "Flagship chat model with thinking mode",
    tags: ["chat", "thinking", "cost-effective"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "deepseek/deepseek-reasoner",
    name: "DeepSeek Reasoner",
    provider: "deepseek",
    contextWindow: 128_000,
    inputCostPer1M: 0.55,
    outputCostPer1M: 2.19,
    modelType: "reasoning",
    capabilities: [
      { name: "reasoning", supported: true },
      { name: "math", supported: true },
      { name: "code", supported: true },
    ],
    description: "Chain-of-thought reasoning specialist",
    tags: ["reasoning", "math", "code"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "anthropic/claude-opus-4-5",
    name: "Claude Opus 4.5",
    provider: "anthropic",
    contextWindow: 200_000,
    inputCostPer1M: 15.0,
    outputCostPer1M: 75.0,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "World's best coding model - SWE-bench: 80.9%",
    tags: ["frontier", "coding", "vision", "best-coding"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "anthropic/claude-sonnet-4-5",
    name: "Claude Sonnet 4.5",
    provider: "anthropic",
    contextWindow: 200_000,
    inputCostPer1M: 3.0,
    outputCostPer1M: 15.0,
    modelType: "code",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "Balanced performance, best value for coding",
    tags: ["balanced", "code", "vision", "value"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "huggingface/Qwen/Qwen3-Next-80B-A3B",
    name: "Qwen3-Next 80B A3B",
    provider: "huggingface",
    contextWindow: 32_000,
    inputCostPer1M: 0.8,
    outputCostPer1M: 0.8,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Frontier open-source reasoning model",
    tags: ["frontier", "open-source", "qwen"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/meta-llama/Llama-3.1-405B",
    name: "Llama 3.1 405B",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 2.7,
    outputCostPer1M: 2.7,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Meta's largest open model",
    tags: ["frontier", "open-source", "llama"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "openrouter/openai/gpt-4-turbo",
    name: "GPT-4 Turbo",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 10.0,
    outputCostPer1M: 30.0,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "OpenAI's flagship model via OpenRouter",
    tags: ["gpt-4", "vision", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "openrouter/google/gemini-2-5-pro",
    name: "Gemini 2.5 Pro",
    provider: "openrouter",
    contextWindow: 1_000_000,
    inputCostPer1M: 1.25,
    outputCostPer1M: 5.0,
    modelType: "vision",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "Google's multimodal frontier model",
    tags: ["frontier", "vision", "1M-context", "gemini"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "nvidia/llama-3.2-nv-embedqa-1b-v2",
    name: "NVIDIA Embedding QA 1B",
    provider: "nvidia",
    contextWindow: 8192,
    inputCostPer1M: 0.02,
    outputCostPer1M: 0.0,
    modelType: "embedding",
    capabilities: [
      { name: "embedding", supported: true },
    ],
    description: "High-performance embedding model",
    tags: ["embedding", "nvidia", "fast"],
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "nvidia/llama-3.2-nv-rerankqa-1b-v2",
    name: "NVIDIA Rerank QA 1B",
    provider: "nvidia",
    contextWindow: 8192,
    inputCostPer1M: 0.03,
    outputCostPer1M: 0.0,
    modelType: "chat",
    capabilities: [
      { name: "reranking", supported: true },
    ],
    description: "Document reranking model",
    tags: ["reranking", "nvidia", "rag"],
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  // ========== Additional Frontier Reasoning Models ==========
  {
    id: "deepseek/deepseek-v3-2-speciale",
    name: "DeepSeek V3.2-Speciale",
    provider: "deepseek",
    contextWindow: 128_000,
    inputCostPer1M: 2.0,
    outputCostPer1M: 8.0,
    modelType: "reasoning",
    capabilities: [
      { name: "reasoning", supported: true },
      { name: "agent", supported: true },
      { name: "tool_calling", supported: true },
    ],
    description: "Advanced reasoning model built for agents (API-only, expires Dec 15, 2025)",
    tags: ["frontier", "agent", "tool-calling", "limited-time"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "deepseek/deepseek-coder-v3-2",
    name: "DeepSeek Coder V3.2",
    provider: "deepseek",
    contextWindow: 128_000,
    inputCostPer1M: 0.14,
    outputCostPer1M: 0.28,
    modelType: "code",
    capabilities: [
      { name: "code", supported: true },
      { name: "debugging", supported: true },
    ],
    description: "Code generation and debugging specialist",
    tags: ["code", "debugging", "cost-effective"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "openai/gpt-5",
    name: "GPT-5",
    provider: "openai",
    contextWindow: 400_000,
    inputCostPer1M: 20.0,
    outputCostPer1M: 60.0,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "reasoning", supported: true },
      { name: "code", supported: true },
    ],
    description: "Dynamic reasoning, 80% fewer hallucinations",
    tags: ["frontier", "reasoning", "400K-context"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/google/gemini-2-5-flash",
    name: "Gemini 2.5 Flash",
    provider: "openrouter",
    contextWindow: 1_000_000,
    inputCostPer1M: 0.35,
    outputCostPer1M: 1.05,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "Fast, cost-effective multimodal model",
    tags: ["fast", "multimodal", "1M-context", "value"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  // ========== Vision & Multimodal Models ==========
  {
    id: "huggingface/Qwen/qwen-image-edit-2511",
    name: "Qwen Image Edit",
    provider: "huggingface",
    contextWindow: 0,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "vision",
    capabilities: [
      { name: "image_editing", supported: true },
      { name: "layered_generation", supported: true },
    ],
    description: "Image editing with layered generation",
    tags: ["vision", "image-editing", "open-source"],
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: true,
  },
  {
    id: "huggingface/Qwen/qwen-image-lightning",
    name: "Qwen Image Lightning",
    provider: "huggingface",
    contextWindow: 0,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "vision",
    capabilities: [
      { name: "image_generation", supported: true },
    ],
    description: "Fast image generation (8-step Lightning workflow)",
    tags: ["vision", "image-generation", "fast", "open-source"],
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: true,
  },
  {
    id: "anthropic/claude-opus-4-5-vision",
    name: "Claude Opus 4.5 Vision",
    provider: "anthropic",
    contextWindow: 200_000,
    inputCostPer1M: 15.0,
    outputCostPer1M: 75.0,
    modelType: "vision",
    capabilities: [
      { name: "text", supported: true },
      { name: "vision", supported: true },
      { name: "ocr", supported: true },
      { name: "pdf_parsing", supported: true },
    ],
    description: "Image understanding and PDF parsing with strong OCR",
    tags: ["frontier", "vision", "ocr", "pdf"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  // ========== Additional HuggingFace Models ==========
  {
    id: "huggingface/Qwen/Qwen3-8B",
    name: "Qwen3 8B",
    provider: "huggingface",
    contextWindow: 32_000,
    inputCostPer1M: 0.05,
    outputCostPer1M: 0.05,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Balanced open-source model for general tasks",
    tags: ["balanced", "open-source", "fast", "qwen"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/Qwen/Qwen3-4B-Instruct",
    name: "Qwen3 4B Instruct",
    provider: "huggingface",
    contextWindow: 32_000,
    inputCostPer1M: 0.02,
    outputCostPer1M: 0.02,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Lightweight instruction-following model",
    tags: ["fast", "open-source", "lightweight", "qwen"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/meta-llama/Llama-3.1-70B",
    name: "Llama 3.1 70B",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 0.9,
    outputCostPer1M: 0.9,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Meta's balanced open model",
    tags: ["open-source", "llama", "balanced"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/meta-llama/Llama-3.1-8B",
    name: "Llama 3.1 8B",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 0.2,
    outputCostPer1M: 0.2,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Lightweight Llama for edge deployment",
    tags: ["open-source", "llama", "lightweight"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/mistralai/Mistral-Large-2",
    name: "Mistral Large 2",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 4.0,
    outputCostPer1M: 4.0,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Mistral's flagship reasoning model",
    tags: ["frontier", "mistral", "code", "open-source"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/mistralai/Mistral-Nemo",
    name: "Mistral Nemo",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 0.3,
    outputCostPer1M: 0.3,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Balanced Mistral model",
    tags: ["balanced", "mistral", "open-source"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/01-ai/Yi-1.5-34B",
    name: "Yi 1.5 34B",
    provider: "huggingface",
    contextWindow: 200_000,
    inputCostPer1M: 0.7,
    outputCostPer1M: 0.7,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "High-quality open model with 200K context",
    tags: ["open-source", "200K-context"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/google/gemma-2-27B",
    name: "Gemma 2 27B",
    provider: "huggingface",
    contextWindow: 2_000_000,
    inputCostPer1M: 0.5,
    outputCostPer1M: 0.5,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Google's open model with 2M context",
    tags: ["open-source", "2M-context", "gemma"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/openai/gpt-oss-120b",
    name: "GPT-OSS 120B",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 2.0,
    outputCostPer1M: 2.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "OpenAI's open-source GPT model",
    tags: ["frontier", "open-source", "gpt"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/openai/gpt-oss-20b",
    name: "GPT-OSS 20B",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 0.4,
    outputCostPer1M: 0.4,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Smaller GPT-OSS variant",
    tags: ["open-source", "gpt", "balanced"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/Qwen/Qwen2.5-Coder-32B",
    name: "Qwen2.5 Coder 32B",
    provider: "huggingface",
    contextWindow: 32_000,
    inputCostPer1M: 0.6,
    outputCostPer1M: 0.6,
    modelType: "code",
    capabilities: [
      { name: "code", supported: true },
    ],
    description: "Code-specialized Qwen model",
    tags: ["code", "open-source", "qwen"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/bigcode/starcoder2",
    name: "StarCoder 2",
    provider: "huggingface",
    contextWindow: 16_000,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "code",
    capabilities: [
      { name: "code", supported: true },
    ],
    description: "Open-source code generation model",
    tags: ["code", "open-source", "free"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/microsoft/Phi-3-mini",
    name: "Phi-3 Mini",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Edge-optimized small model",
    tags: ["edge", "open-source", "free", "phi"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/NousResearch/Hermes-3-70B",
    name: "Hermes 3 70B",
    provider: "huggingface",
    contextWindow: 128_000,
    inputCostPer1M: 0.9,
    outputCostPer1M: 0.9,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Nous Research's flagship open model",
    tags: ["open-source", "nous"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/cognitivecomputations/dolphin",
    name: "Dolphin",
    provider: "huggingface",
    contextWindow: 32_000,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "Community-trained chat model",
    tags: ["chat", "open-source", "free"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  // ========== OpenRouter Additional Models ==========
  {
    id: "openrouter/openai/gpt-4o",
    name: "GPT-4o",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 2.5,
    outputCostPer1M: 10.0,
    modelType: "vision",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "OpenAI's omni model",
    tags: ["gpt-4", "vision", "multimodal", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "openrouter/openai/gpt-4o-mini",
    name: "GPT-4o Mini",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 0.15,
    outputCostPer1M: 0.6,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "Fast, cost-effective GPT-4o variant",
    tags: ["fast", "value", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "openrouter/anthropic/claude-3-5-sonnet",
    name: "Claude 3.5 Sonnet (via OpenRouter)",
    provider: "openrouter",
    contextWindow: 200_000,
    inputCostPer1M: 3.0,
    outputCostPer1M: 15.0,
    modelType: "code",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
      { name: "vision", supported: true },
    ],
    description: "Claude Sonnet via OpenRouter gateway",
    tags: ["balanced", "code", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: true,
  },
  {
    id: "openrouter/meta-llama/llama-3.3-70b",
    name: "Llama 3.3 70B (via OpenRouter)",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 0.35,
    outputCostPer1M: 0.4,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Latest Llama via OpenRouter",
    tags: ["open-source", "llama", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/mistralai/mistral-large",
    name: "Mistral Large (via OpenRouter)",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 4.0,
    outputCostPer1M: 12.0,
    modelType: "reasoning",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Mistral's frontier model via OpenRouter",
    tags: ["mistral", "frontier", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/cohere/command-r-plus",
    name: "Command R Plus",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 3.0,
    outputCostPer1M: 15.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "rag", supported: true },
    ],
    description: "RAG-optimized model from Cohere",
    tags: ["rag", "retrieval", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/perplexity/llama-3.1-sonar-small",
    name: "Perplexity Sonar Small",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 0.2,
    outputCostPer1M: 0.2,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "search", supported: true },
    ],
    description: "Search-augmented model",
    tags: ["search", "perplexity", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "openrouter/qwen/qwen-2.5-72b-instruct",
    name: "Qwen 2.5 72B Instruct",
    provider: "openrouter",
    contextWindow: 32_000,
    inputCostPer1M: 0.35,
    outputCostPer1M: 0.4,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
      { name: "code", supported: true },
    ],
    description: "Alibaba's frontier model via OpenRouter",
    tags: ["frontier", "qwen", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/01-ai/yi-large",
    name: "Yi Large",
    provider: "openrouter",
    contextWindow: 200_000,
    inputCostPer1M: 3.0,
    outputCostPer1M: 3.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "01.AI's frontier model",
    tags: ["frontier", "200K-context", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/deepseek/deepseek-chat",
    name: "DeepSeek Chat (via OpenRouter)",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 0.14,
    outputCostPer1M: 0.28,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "DeepSeek accessible via OpenRouter",
    tags: ["chat", "cost-effective", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  {
    id: "openrouter/xai/grok-beta",
    name: "Grok Beta (via OpenRouter)",
    provider: "openrouter",
    contextWindow: 128_000,
    inputCostPer1M: 5.0,
    outputCostPer1M: 15.0,
    modelType: "chat",
    capabilities: [
      { name: "text", supported: true },
    ],
    description: "xAI Grok accessible via OpenRouter",
    tags: ["chat", "xai", "openrouter"],
    supportsStreaming: true,
    supportsFunctionCalling: true,
    supportsVision: false,
  },
  // ========== Embedding Models ==========
  {
    id: "openai/text-embedding-3-large",
    name: "OpenAI Embeddings Large",
    provider: "openai",
    contextWindow: 8191,
    inputCostPer1M: 0.13,
    outputCostPer1M: 0.0,
    modelType: "embedding",
    capabilities: [
      { name: "embedding", supported: true },
    ],
    description: "3072-dim embeddings for high precision",
    tags: ["embedding", "3072-dim", "high-precision"],
    maxTokens: 8191,
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "openai/text-embedding-3-small",
    name: "OpenAI Embeddings Small",
    provider: "openai",
    contextWindow: 8191,
    inputCostPer1M: 0.02,
    outputCostPer1M: 0.0,
    modelType: "embedding",
    capabilities: [
      { name: "embedding", supported: true },
    ],
    description: "1536-dim embeddings, cost-effective",
    tags: ["embedding", "1536-dim", "value"],
    maxTokens: 8191,
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/BAAI/bge-large-en-v1.5",
    name: "BGE Large v1.5",
    provider: "huggingface",
    contextWindow: 512,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "embedding",
    capabilities: [
      { name: "embedding", supported: true },
    ],
    description: "1024-dim open-source embeddings",
    tags: ["embedding", "1024-dim", "open-source", "free"],
    maxTokens: 512,
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
  {
    id: "huggingface/jinaai/jina-embeddings-v3",
    name: "Jina Embeddings v3",
    provider: "huggingface",
    contextWindow: 8192,
    inputCostPer1M: 0.0,
    outputCostPer1M: 0.0,
    modelType: "embedding",
    capabilities: [
      { name: "embedding", supported: true },
      { name: "multilingual", supported: true },
    ],
    description: "1024-dim multilingual embeddings (100+ languages)",
    tags: ["embedding", "1024-dim", "multilingual", "free"],
    maxTokens: 8192,
    supportsStreaming: false,
    supportsFunctionCalling: false,
    supportsVision: false,
  },
];

export const getModelsByProvider = (provider: Model["provider"]) => {
  return UNIFIED_MODEL_CATALOG.filter((m) => m.provider === provider);
};

export const getModelById = (id: string) => {
  return UNIFIED_MODEL_CATALOG.find((m) => m.id === id);
};

export const getModelsByType = (type: Model["modelType"]) => {
  return UNIFIED_MODEL_CATALOG.filter((m) => m.modelType === type);
};

export const getModelsByTag = (tag: string) => {
  return UNIFIED_MODEL_CATALOG.filter((m) => m.tags.includes(tag));
};
