# üöÄ AI Integration Platform - Complete Master Guide

## Executive Summary

This is a **production-ready, comprehensive AI Integration Platform** that unifies access to **70+ AI models** from **12+ providers** including mainstream services (OpenAI, Anthropic, Google, xAI, DeepSeek) and **uncensored/privacy-focused providers** (Venice AI, DeepInfra, Hermes, Dolphin).

### Key Features
‚úÖ **Unified Model Catalog** - 70+ models from all major providers  
‚úÖ **Live API Testing** - Real-time testing with streaming responses  
‚úÖ **Batch Comparison** - Compare responses from multiple models simultaneously  
‚úÖ **RAG Pipeline** - Complete scraping ‚Üí embedding ‚Üí retrieval ‚Üí generation  
‚úÖ **Uncensored Models** - Privacy-first models with zero-logging  
‚úÖ **API Key Management** - Secure, browser-only storage with validation  
‚úÖ **Export/Import** - Configuration templates for deployment  
‚úÖ **Interactive SDK Demos** - Live integration examples for all providers

---

## üìä Complete Model Catalog (70+ Models)

### Frontier Reasoning Models

| Model ID | Provider | Context | Best For | Pricing (/1M) |
|----------|----------|---------|----------|---------------|
| `xai/grok-4-1-fast-reasoning` | xAI | 2M | Complex reasoning, agentic workflows | $15 in / $30 out |
| `xai/grok-4-1-fast-non-reasoning` | xAI | 2M | Quick responses, real-time chat | $8 in / $16 out |
| `deepseek/deepseek-reasoner` | DeepSeek | 128K | Chain-of-thought reasoning | $0.55 in / $2.19 out |
| `anthropic/claude-opus-4-5` | Anthropic | 200K | Software engineering (SWE-bench: 80.9%) | $15 in / $75 out |
| `openrouter/openai/gpt-4-turbo` | OpenAI | 128K | General reasoning, coding | $10 in / $30 out |

### Specialized Coding Models

| Model ID | Provider | Context | Best For | Pricing (/1M) |
|----------|----------|---------|----------|---------------|
| `xai/grok-code-fast-1` | xAI | 2M | Lightning-fast code completion | $12 in / $24 out |
| `deepseek/deepseek-coder` | DeepSeek | 128K | Code generation, debugging | $0.14 in / $0.28 out |
| `anthropic/claude-sonnet-4-5` | Anthropic | 200K | Balanced coding performance | $3 in / $15 out |

### Vision & Multimodal

| Model ID | Provider | Context | Best For | Pricing (/1M) |
|----------|----------|---------|----------|---------------|
| `xai/grok-2-vision-latest` | xAI | 2M | Image analysis with web search | $10 in / $20 out |
| `anthropic/claude-opus-4-5` | Anthropic | 200K | Image understanding, PDF parsing | $15 in / $75 out |
| `openrouter/google/gemini-2-5-pro` | Google | 1M | Multimodal, long documents | $1.25 in / $5 out |

### Embeddings & RAG

| Model ID | Provider | Dimensions | Best For | Pricing (/1M) |
|----------|----------|------------|----------|---------------|
| `openai/text-embedding-3-large` | OpenAI | 3072 | General RAG, semantic search | $0.13 |
| `huggingface/BAAI/bge-large-en-v1.5` | HuggingFace | 1024 | Cost-effective RAG | **FREE** |
| `nvidia/llama-3.2-nv-embedqa-1b-v2` | NVIDIA | N/A | Fast, optimized embeddings | $0.02 |

### üîì Uncensored & Privacy-First Models

| Model ID | Provider | Context | Privacy | Pricing (/1M) |
|----------|----------|---------|---------|---------------|
| `venice/venice-uncensored` | Venice AI | 32K | **Zero-Logging** | $0.5 in / $0.5 out |
| `openrouter/dolphin-mistral-24b:free` | OpenRouter | 32K | Standard | **FREE** |
| `openrouter/dolphin3.0-r1:free` | OpenRouter | 32K | Standard | **FREE** |
| `deepinfra/dolphin-2.6-mixtral-8x7b` | DeepInfra | 32K | **Zero-Logging** | $0.2 in / $0.2 out |
| `huggingface/Hermes-4.3-36B` | HuggingFace | **512K** | Standard | $0.4 in / $0.4 out |
| `huggingface/Hermes-3-405B` | HuggingFace | 128K | Standard | $2.7 in / $2.7 out |
| `huggingface/Dolphin3.0-Llama3.1-8B` | HuggingFace | 32K | Standard | $0.05 in / $0.05 out |

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      AI INTEGRATION PLATFORM                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ  Frontend   ‚îÇ  ‚îÇ   Backend   ‚îÇ  ‚îÇ    Agents   ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ (React 19)  ‚îÇ  ‚îÇ  (Node.js)  ‚îÇ  ‚îÇ (LangGraph) ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ + shadcn/ui ‚îÇ  ‚îÇ  + Express  ‚îÇ  ‚îÇ + MCP Tools ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ               UNIFIED MODEL GATEWAY                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (Route requests to correct provider + normalize)         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                            ‚îÇ                                   ‚îÇ
‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ      ‚ñº          ‚ñº          ‚ñº          ‚ñº          ‚ñº            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  xAI   ‚îÇ ‚îÇDeepSeek‚îÇ ‚îÇAnthrop.‚îÇ ‚îÇOpenRtr.‚îÇ ‚îÇ Venice ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ Grok 4 ‚îÇ ‚îÇ  V3.2  ‚îÇ ‚îÇOpus 4.5‚îÇ ‚îÇ 400+   ‚îÇ ‚îÇUncens. ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                  RAG INFRASTRUCTURE                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Firecrawl ‚Üí Supabase (pgvector) ‚Üí LiteLLM               ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Quick Start

### 1. Environment Setup

Create `.env` file:

```bash
# ========================================
# CORE PROVIDERS (Required)
# ========================================
OPENROUTER_API_KEY="sk-or-v1-xxxxx"           # 400+ models + FREE uncensored
XAI_API_KEY="xai-xxxxx"                        # Grok 4.1 (2M context)
DEEPSEEK_API_KEY="sk-xxxxx"                    # V3.2 & Reasoner

# ========================================
# PREMIUM MODELS (Optional)
# ========================================
ANTHROPIC_API_KEY="sk-ant-xxxxx"               # Claude Opus 4.5
OPENAI_API_KEY="sk-xxxxx"                      # GPT-4 Turbo
GOOGLE_API_KEY="AIza-xxxxx"                    # Gemini 2.5 Pro

# ========================================
# UNCENSORED PROVIDERS (Optional)
# ========================================
VENICE_API_KEY="venice-xxxxx"                  # Zero-logging uncensored
DEEPINFRA_API_KEY="xxxxx"                      # Dolphin Mixtral
HUGGINGFACE_TOKEN="hf_xxxxx"                   # Hermes, Dolphin

# ========================================
# RAG INFRASTRUCTURE (Optional)
# ========================================
FIRECRAWL_API_KEY="fc-xxxxx"                   # Web scraping
PINECONE_API_KEY="xxxxx"                       # Vector database
NVIDIA_NIM_API_KEY="nvapi-xxxxx"               # Embeddings/Rerank
```

### 2. Installation

```bash
# Install dependencies
npm install

# Start development server
npm run dev

# Build for production
npm run build
```

### 3. Access the Platform

Open `http://localhost:5173` and navigate through:

1. **Catalog Tab** - Browse all 70+ models
2. **Config Tab** - Add and validate API keys
3. **Live Tab** - Test models with real API calls
4. **Batch Tab** - Compare multiple models side-by-side
5. **RAG Tab** - Full RAG pipeline demo

---

## üìñ Provider Integration Guides

### xAI Grok 4.1 (You have Heavy API key)

**Models Available:**
- `grok-4-1-fast-reasoning` - 2M context, reasoning tokens
- `grok-4-1-fast-non-reasoning` - Same model, no reasoning overhead
- `grok-code-fast-1` - Specialized for code generation
- `grok-2-vision-latest` - Multimodal with image support

**Key Features:**
- ‚úÖ 2M token context window
- ‚úÖ Web search integration
- ‚úÖ Function/tool calling
- ‚úÖ Real-time X platform integration

**Usage Example:**
```typescript
const response = await fetch('https://api.x.ai/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${XAI_API_KEY}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'grok-4-1-fast-reasoning',
    messages: [{ role: 'user', content: 'Explain quantum entanglement' }],
    temperature: 0.7
  })
});
```

**Critical Notes:**
- ‚ùå **Reasoning models do NOT support:**
  - `presencePenalty`
  - `frequencyPenalty`
  - `stop` sequences
  - `reasoning_effort` parameter
- ‚úÖ **Non-reasoning models support all standard OpenAI parameters**

---

### Venice AI (Zero-Logging Uncensored)

**Models Available:**
- `venice-uncensored` - 32K context, zero data retention

**Key Features:**
- ‚úÖ **Zero-logging policy** - No data retention
- ‚úÖ Uncensored responses
- ‚úÖ OpenAI-compatible API
- ‚úÖ Privacy-first architecture

**Usage Example:**
```typescript
const response = await fetch('https://api.venice.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${VENICE_API_KEY}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'venice-uncensored',
    messages: [{ role: 'user', content: 'Your query here' }]
  })
});
```

---

### OpenRouter (400+ Models + Free Tier)

**Free Uncensored Models:**
- `cognitivecomputations/dolphin-mistral-24b-venice-edition:free`
- `cognitivecomputations/dolphin3.0-r1-mistral-24b:free`

**Usage Example:**
```typescript
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
    'HTTP-Referer': 'https://your-app.com',
    'X-Title': 'AI Platform'
  },
  body: JSON.stringify({
    model: 'cognitivecomputations/dolphin-mistral-24b-venice-edition:free',
    messages: [{ role: 'user', content: 'Test prompt' }]
  })
});
```

---

### DeepSeek (Cost-Effective Reasoning)

**Models Available:**
- `deepseek-chat` - $0.14/$0.28 per 1M
- `deepseek-reasoner` - $0.55/$2.19 per 1M (chain-of-thought)
- `deepseek-coder` - $0.14/$0.28 per 1M

**Usage Example:**
```typescript
const response = await fetch('https://api.deepseek.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${DEEPSEEK_API_KEY}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'deepseek-reasoner',
    messages: [{ role: 'user', content: 'Solve this math problem: ...' }]
  })
});
```

---

## üß© RAG Pipeline Architecture

### Complete Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RAG PIPELINE STAGES                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1. SCRAPING (Firecrawl/Oxylabs)                               ‚îÇ
‚îÇ     ‚îî‚îÄ‚ñ∫ Extract ‚Üí Clean ‚Üí Chunk (semantic)                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. EMBEDDING (OpenAI/HuggingFace/NVIDIA)                      ‚îÇ
‚îÇ     ‚îî‚îÄ‚ñ∫ Generate vectors ‚Üí Store in Supabase pgvector         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. RETRIEVAL (Hybrid Search)                                  ‚îÇ
‚îÇ     ‚îî‚îÄ‚ñ∫ Vector similarity + Keyword filtering                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  4. RERANKING (NVIDIA NIM/Cohere)                             ‚îÇ
‚îÇ     ‚îî‚îÄ‚ñ∫ Improve relevance with cross-encoder                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  5. GENERATION (LiteLLM ‚Üí Any Model)                           ‚îÇ
‚îÇ     ‚îî‚îÄ‚ñ∫ Augment prompt ‚Üí Stream response ‚Üí Cite sources        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation Example

```typescript
// 1. Scrape content
const scraped = await firecrawl.scrape({
  url: 'https://example.com/docs',
  formats: ['markdown']
});

// 2. Generate embeddings
const embedding = await openai.embeddings.create({
  model: 'text-embedding-3-large',
  input: scraped.content
});

// 3. Store in Supabase
await supabase.from('documents').insert({
  content: scraped.content,
  embedding: embedding.data[0].embedding,
  metadata: { url: scraped.url }
});

// 4. Query with RAG
const queryEmbedding = await openai.embeddings.create({
  model: 'text-embedding-3-large',
  input: 'What is RAG?'
});

const { data: results } = await supabase.rpc('match_documents', {
  query_embedding: queryEmbedding.data[0].embedding,
  match_count: 5
});

// 5. Generate answer
const context = results.map(r => r.content).join('\n\n');
const completion = await litellm.completion({
  model: 'grok-4-1-fast-reasoning',
  messages: [{
    role: 'user',
    content: `Context:\n${context}\n\nQuestion: What is RAG?\nAnswer with citations:`
  }]
});
```

---

## üîí Security & Privacy

### API Key Storage

‚úÖ **Browser-only storage** - Keys never sent to servers  
‚úÖ **localStorage encryption** - AES-256 encryption layer  
‚úÖ **Validation before storage** - Test keys before saving  
‚úÖ **Per-session expiry** - Keys cleared on browser close (optional)

### Privacy-First Providers

| Provider | Data Retention | Logging | Use Case |
|----------|----------------|---------|----------|
| Venice AI | **Zero** | None | Maximum privacy |
| DeepInfra | **Zero** | None | Fast uncensored |
| HuggingFace | Standard | Usage stats | Open-source models |
| OpenRouter | Standard | Request logs | Multi-model gateway |

---

## üì¶ Export/Import Configurations

### Export Your Setup

```typescript
// Export all configurations
const config = {
  apiKeys: {
    openrouter: 'sk-or-...',
    xai: 'xai-...',
    // ... (encrypted before export)
  },
  favorites: ['xai/grok-4-1-fast-reasoning', 'venice/venice-uncensored'],
  presets: [
    {
      name: 'Code Review',
      model: 'anthropic/claude-opus-4-5',
      temperature: 0.3,
      maxTokens: 4096
    }
  ]
};

// Download as JSON
downloadJSON(config, 'ai-platform-config.json');
```

### Import Configuration

```typescript
// Upload JSON file
const config = await uploadJSON('ai-platform-config.json');

// Restore settings
restoreAPIKeys(config.apiKeys);
restoreFavorites(config.favorites);
restorePresets(config.presets);
```

---

## üéØ Use Cases & Examples

### 1. Code Review with Claude Opus 4.5

```typescript
const codeReview = await modelRouter.completion({
  modelId: 'anthropic/claude-opus-4-5',
  messages: [{
    role: 'user',
    content: `Review this code for security issues:\n\n${codeSnippet}`
  }],
  temperature: 0.2
});
```

### 2. Complex Reasoning with Grok 4.1

```typescript
const reasoning = await modelRouter.completion({
  modelId: 'xai/grok-4-1-fast-reasoning',
  messages: [{
    role: 'user',
    content: 'Analyze the Monty Hall problem step-by-step with web search'
  }],
  reasoning_effort: 'high'  // ‚ùå NOT SUPPORTED - remove this param
});
```

### 3. Uncensored Creative Writing with Venice

```typescript
const creative = await modelRouter.completion({
  modelId: 'venice/venice-uncensored',
  messages: [{
    role: 'user',
    content: 'Write a cyberpunk short story with mature themes'
  }],
  temperature: 0.9
});
```

### 4. Batch Model Comparison

```typescript
const models = [
  'xai/grok-4-1-fast-reasoning',
  'anthropic/claude-sonnet-4-5',
  'deepseek/deepseek-reasoner',
  'openrouter/dolphin-mistral-24b:free'
];

const results = await Promise.all(
  models.map(model => modelRouter.completion({
    modelId: model,
    messages: [{ role: 'user', content: 'Explain quantum computing' }]
  }))
);

// Compare responses side-by-side
displayComparison(results);
```

---

## üöÄ Deployment

### Vercel (Recommended)

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
vercel --prod

# Set environment variables
vercel env add OPENROUTER_API_KEY production
vercel env add XAI_API_KEY production
vercel env add VENICE_API_KEY production
```

### Docker

```dockerfile
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

```bash
docker build -t ai-platform .
docker run -p 3000:3000 --env-file .env ai-platform
```

---

## üìä Cost Optimization

### Model Selection Guide

| Use Case | Recommended Model | Cost (/1M) | Reason |
|----------|-------------------|------------|--------|
| Quick Q&A | `deepseek-chat` | $0.14/$0.28 | 10x cheaper than GPT-4 |
| Code Generation | `xai/grok-code-fast-1` | $12/$24 | Fast + large context |
| Creative Writing | `venice/venice-uncensored` | $0.5/$0.5 | Uncensored + private |
| **Free Tier** | `openrouter/dolphin:free` | **$0/$0** | No cost |
| Complex Reasoning | `xai/grok-4-1-fast-reasoning` | $15/$30 | 2M context |
| Best Coding | `anthropic/claude-opus-4-5` | $15/$75 | SWE-bench #1 |

### Cost Reduction Strategies

1. **Use DeepSeek for 90% of tasks** ($0.14/$0.28)
2. **Free tier for testing** (Dolphin on OpenRouter)
3. **Cache frequent queries** (Redis + 24h TTL)
4. **Smart routing** - Simple questions ‚Üí cheap models
5. **Batch processing** - Group requests to save overhead

---

## üõ†Ô∏è Troubleshooting

### Common Issues

#### ‚ùå "Rate limit exceeded"
**Solution:** Implement client-side rate limiting or upgrade API tier.

```typescript
const rateLimiter = new RateLimiter({
  tokensPerInterval: 10,
  interval: 'minute'
});

await rateLimiter.removeTokens(1);
```

#### ‚ùå "Invalid API key"
**Solution:** Validate keys before storage.

```typescript
async function validateKey(provider: string, key: string): Promise<boolean> {
  const endpoints = {
    openrouter: 'https://openrouter.ai/api/v1/models',
    xai: 'https://api.x.ai/v1/chat/completions',
    venice: 'https://api.venice.ai/api/v1/chat/completions'
  };
  
  try {
    const res = await fetch(endpoints[provider], {
      headers: { 'Authorization': `Bearer ${key}` }
    });
    return res.ok;
  } catch {
    return false;
  }
}
```

#### ‚ùå Grok reasoning model parameter errors
**Solution:** Remove unsupported params.

```typescript
// ‚ùå WRONG
{
  model: 'grok-4-1-fast-reasoning',
  stop: ['\n\n'],              // NOT SUPPORTED
  presencePenalty: 0.5,        // NOT SUPPORTED
  frequencyPenalty: 0.5        // NOT SUPPORTED
}

// ‚úÖ CORRECT
{
  model: 'grok-4-1-fast-reasoning',
  temperature: 0.7,
  max_tokens: 4096
}
```

---

## üìö Additional Resources

### Documentation Links
- [xAI Grok Documentation](https://docs.x.ai/docs)
- [Venice AI Docs](https://www.venice.ai)
- [OpenRouter Models](https://openrouter.ai/models)
- [DeepSeek API](https://platform.deepseek.com/docs)
- [Anthropic Claude](https://docs.anthropic.com/)
- [LiteLLM Gateway](https://docs.litellm.ai/)
- [Firecrawl API](https://docs.firecrawl.dev/)

### Community & Support
- GitHub Issues: Report bugs and feature requests
- Discord: Join the community
- Email: support@aiplatform.dev

---

## üéì Next Steps

1. ‚úÖ **Configure API Keys** - Add at least OpenRouter + xAI
2. ‚úÖ **Test Live Models** - Use the Live tab to test responses
3. ‚úÖ **Try Batch Comparison** - Compare 3-4 models side-by-side
4. ‚úÖ **Explore Uncensored** - Test Venice AI for privacy-first usage
5. ‚úÖ **Build RAG Pipeline** - Scrape ‚Üí Embed ‚Üí Retrieve ‚Üí Generate
6. ‚úÖ **Export Configuration** - Save your setup for deployment

---

**Built with ‚ù§Ô∏è by the AI Integration Platform Team**

*Version 2.0.0 | Last Updated: December 2025*
